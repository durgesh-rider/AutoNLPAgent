{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoNLP-Agent: No-Code NLP Platform\n",
    "\n",
    "A comprehensive NLP platform that automatically detects tasks, preprocesses data, trains models, and evaluates performance.\n",
    "\n",
    "## Features\n",
    "- üîç Automatic NLP task detection\n",
    "- üßπ Data preprocessing pipeline\n",
    "- ü§ñ ML model training (Scikit-learn & Transformers)\n",
    "- üìä Model evaluation with visualizations\n",
    "- üéØ GPU acceleration support\n",
    "\n",
    "## Supported Tasks\n",
    "- Text Classification\n",
    "- Sentiment Analysis\n",
    "- Named Entity Recognition\n",
    "- Question Answering\n",
    "- Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy scikit-learn transformers torch nltk spacy\n",
    "!pip install matplotlib seaborn plotly\n",
    "!pip install openpyxl xlrd  # For Excel file support\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Download SpaCy model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"AutoNLP-Agent initialized successfully!\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload Your Dataset\n",
    "\n",
    "Upload your CSV, TXT, or Excel file containing text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload file\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    print(f\"Uploaded file: {filename}\")\n",
    "    \n",
    "    # Determine file type and load data\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "    elif filename.endswith(('.xlsx', '.xls')):\n",
    "        df = pd.read_excel(filename)\n",
    "    elif filename.endswith('.txt'):\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nColumn info:\")\n",
    "    print(df.info())\n",
    "else:\n",
    "    print(\"No file uploaded. Using sample data instead.\")\n",
    "    \n",
    "    # Sample sentiment analysis data\n",
    "    sample_data = {\n",
    "        'text': [\n",
    "            'I love this product! It works perfectly.',\n",
    "            'This is amazing quality and great value.',\n",
    "            'Excellent customer service and fast delivery.',\n",
    "            'Terrible product, complete waste of money.',\n",
    "            'Poor quality and bad customer support.',\n",
    "            'Awful experience, never buying again.',\n",
    "            'Good product but arrived late.',\n",
    "            'Decent quality for the price.',\n",
    "            'Not bad, does what it says.',\n",
    "            'Fantastic! Exceeded my expectations.'\n",
    "        ],\n",
    "        'sentiment': [\n",
    "            'positive', 'positive', 'positive',\n",
    "            'negative', 'negative', 'negative',\n",
    "            'neutral', 'neutral', 'neutral', 'positive'\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(\"Using sample sentiment analysis data\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Automatic Task Detection\n",
    "\n",
    "The system automatically detects what type of NLP task your data represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nlp_task(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Automatically detect the NLP task type from the dataset structure and content.\"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Check for sentiment-related keywords\n",
    "    sentiment_keywords = ['sentiment', 'polarity', 'emotion', 'feeling']\n",
    "    if any(keyword in ' '.join(columns).lower() for keyword in sentiment_keywords):\n",
    "        return 'sentiment_analysis'\n",
    "    \n",
    "    # Check for classification patterns\n",
    "    text_columns = []\n",
    "    label_columns = []\n",
    "    \n",
    "    for col in columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in ['text', 'content', 'review', 'comment']):\n",
    "            text_columns.append(col)\n",
    "        elif any(keyword in col_lower for keyword in ['label', 'target', 'class', 'category']):\n",
    "            label_columns.append(col)\n",
    "    \n",
    "    if text_columns and label_columns:\n",
    "        return 'classification'\n",
    "    \n",
    "    # Check content for sentiment indicators\n",
    "    if len(columns) >= 2:\n",
    "        sample_df = df.head(min(50, len(df)))\n",
    "        \n",
    "        for col in columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                sample_texts = sample_df[col].dropna().astype(str).tolist()[:10]\n",
    "                \n",
    "                positive_words = ['good', 'great', 'excellent', 'amazing', 'love', 'best']\n",
    "                negative_words = ['bad', 'terrible', 'awful', 'hate', 'worst', 'poor']\n",
    "                \n",
    "                has_sentiment = False\n",
    "                for text in sample_texts:\n",
    "                    text_lower = text.lower()\n",
    "                    if any(word in text_lower for word in positive_words + negative_words):\n",
    "                        has_sentiment = True\n",
    "                        break\n",
    "                \n",
    "                if has_sentiment:\n",
    "                    return 'sentiment_analysis'\n",
    "    \n",
    "    # Default to classification\n",
    "    return 'classification'\n",
    "\n",
    "# Detect task\n",
    "detected_task = detect_nlp_task(df)\n",
    "print(f\"üîç Detected NLP Task: {detected_task.upper()}\")\n",
    "\n",
    "# Identify columns\n",
    "text_col = None\n",
    "label_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if df[col].dtype == 'object' and not text_col:\n",
    "        text_col = col\n",
    "    elif not label_col and col != text_col:\n",
    "        label_col = col\n",
    "\n",
    "print(f\"üìù Text Column: {text_col}\")\n",
    "print(f\"üè∑Ô∏è  Label Column: {label_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n",
    "\n",
    "Clean and prepare your text data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.label_encoder = LabelEncoder()\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess individual text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        import nltk\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess_dataset(self, df: pd.DataFrame, text_col: str, label_col: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "        \"\"\"Preprocess the entire dataset\"\"\"\n",
    "        print(\"üßπ Starting data preprocessing...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.dropna(subset=[text_col, label_col])\n",
    "        print(f\"‚úÖ Handled missing values. Remaining rows: {len(df)}\")\n",
    "        \n",
    "        # Preprocess text\n",
    "        print(\"üìù Preprocessing text data...\")\n",
    "        df[f'{text_col}_processed'] = df[text_col].apply(self.preprocess_text)\n",
    "        \n",
    "        # Encode labels\n",
    "        df[f'{label_col}_encoded'] = self.label_encoder.fit_transform(df[label_col])\n",
    "        print(f\"üè∑Ô∏è  Encoded {len(self.label_encoder.classes_)} classes: {list(self.label_encoder.classes_)}\")\n",
    "        \n",
    "        # Add text features\n",
    "        df[f'{text_col}_length'] = df[text_col].apply(len)\n",
    "        df[f'{text_col}_word_count'] = df[text_col].apply(lambda x: len(str(x).split()))\n",
    "        \n",
    "        metadata = {\n",
    "            'original_shape': df.shape,\n",
    "            'text_column': text_col,\n",
    "            'label_column': label_col,\n",
    "            'processed_text_column': f'{text_col}_processed',\n",
    "            'encoded_label_column': f'{label_col}_encoded',\n",
    "            'classes': list(self.label_encoder.classes_),\n",
    "            'num_classes': len(self.label_encoder.classes_)\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Preprocessing completed!\")\n",
    "        return df, metadata\n",
    "\n",
    "# Preprocess data\n",
    "preprocessor = TextPreprocessor()\n",
    "processed_df, metadata = preprocessor.preprocess_dataset(df, text_col, label_col)\n",
    "\n",
    "print(\"\\nüìä Processed Dataset Info:\")\n",
    "print(f\"Shape: {processed_df.shape}\")\n",
    "print(f\"Classes: {metadata['classes']}\")\n",
    "print(\"\\nSample processed data:\")\n",
    "print(processed_df[[text_col, f'{text_col}_processed', label_col, f'{label_col}_encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "\n",
    "Train a machine learning model on your processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.vectorizers = {}\n",
    "    \n",
    "    def train_sklearn_model(self, X_train, y_train, model_type='logistic'):\n",
    "        \"\"\"Train a scikit-learn model\"\"\"\n",
    "        print(f\"ü§ñ Training {model_type} model...\")\n",
    "        \n",
    "        # Vectorize text\n",
    "        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        \n",
    "        # Choose model\n",
    "        if model_type == 'logistic':\n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        elif model_type == 'random_forest':\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        else:\n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        \n",
    "        model_id = f\"sklearn_{model_type}_{datetime.now().strftime('%H%M%S')}\"\n",
    "        self.models[model_id] = model\n",
    "        self.vectorizers[model_id] = vectorizer\n",
    "        \n",
    "        print(\"‚úÖ Model trained successfully!\")\n",
    "        return model_id, model, vectorizer\n",
    "    \n",
    "    def train_transformer_model(self, X_train, y_train, num_labels):\n",
    "        \"\"\"Train a transformer model\"\"\"\n",
    "        print(\"üöÄ Training transformer model (this may take a while)...\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        model_name = \"distilbert-base-uncased\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels\n",
    "        )\n",
    "        \n",
    "        # Prepare dataset\n",
    "        class TextDataset(Dataset):\n",
    "            def __init__(self, texts, labels, tokenizer):\n",
    "                self.texts = texts\n",
    "                self.labels = labels\n",
    "                self.tokenizer = tokenizer\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.texts)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                text = self.texts[idx]\n",
    "                label = self.labels[idx]\n",
    "                \n",
    "                encoding = tokenizer(\n",
    "                    text,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=256,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    'input_ids': encoding['input_ids'].flatten(),\n",
    "                    'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                    'labels': torch.tensor(label, dtype=torch.long)\n",
    "                }\n",
    "        \n",
    "        train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            warmup_steps=100,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=50,\n",
    "            save_steps=500,\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_strategy=\"no\",\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer.train()\n",
    "        \n",
    "        model_id = f\"transformer_{datetime.now().strftime('%H%M%S')}\"\n",
    "        self.models[model_id] = {\n",
    "            'model': model,\n",
    "            'tokenizer': tokenizer,\n",
    "            'trainer': trainer\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Transformer model trained successfully!\")\n",
    "        return model_id\n",
    "\n",
    "# Prepare training data\n",
    "X = processed_df[metadata['processed_text_column']]\n",
    "y = processed_df[metadata['encoded_label_column']]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"üìä Training set: {len(X_train)} samples\")\n",
    "print(f\"üìä Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Choose model type based on data size\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "if len(X_train) < 1000:\n",
    "    print(\"üìè Small dataset detected - using scikit-learn model\")\n",
    "    model_id, model, vectorizer = trainer.train_sklearn_model(X_train, y_train, 'logistic')\n",
    "    model_type = 'sklearn'\n",
    "else:\n",
    "    print(\"üìè Large dataset detected - using transformer model\")\n",
    "    model_id = trainer.train_transformer_model(X_train, y_train, metadata['num_classes'])\n",
    "    model_type = 'transformer'\n",
    "\n",
    "print(f\"üéØ Model trained with ID: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation\n",
    "\n",
    "Evaluate your trained model with comprehensive metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_id: str, X_test, y_test, model_type: str):\n",
    "    \"\"\"Evaluate the trained model\"\"\"\n",
    "    print(\"üìä Evaluating model performance...\")\n",
    "    \n",
    "    if model_type == 'sklearn':\n",
    "        model = trainer.models[model_id]\n",
    "        vectorizer = trainer.vectorizers[model_id]\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "    else:\n",
    "        # Transformer model\n",
    "        model_info = trainer.models[model_id]\n",
    "        model = model_info['model']\n",
    "        tokenizer = model_info['tokenizer']\n",
    "        \n",
    "        y_pred = []\n",
    "        for text in X_test:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "                y_pred.append(pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(\"‚úÖ Evaluation completed!\")\n",
    "    return metrics, cm, y_pred\n",
    "\n",
    "# Evaluate model\n",
    "metrics, confusion_matrix, y_pred = evaluate_model(model_id, X_test, y_test, model_type)\n",
    "\n",
    "print(\"\\nüìà Model Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")\n",
    "\n",
    "# Decode predictions for display\n",
    "y_test_decoded = preprocessor.label_encoder.inverse_transform(y_test)\n",
    "y_pred_decoded = preprocessor.label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test_decoded, y_pred_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualizations\n",
    "\n",
    "Explore your results with interactive charts and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "def create_visualizations(metrics: Dict, confusion_matrix, y_test_decoded, y_pred_decoded):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    \n",
    "    # 1. Metrics Bar Chart\n",
    "    fig_metrics = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=list(metrics.keys()),\n",
    "            y=list(metrics.values()),\n",
    "            marker_color='lightblue'\n",
    "        )\n",
    "    ])\n",
    "    fig_metrics.update_layout(\n",
    "        title='Model Performance Metrics',\n",
    "        xaxis_title='Metric',\n",
    "        yaxis_title='Score',\n",
    "        yaxis_range=[0, 1]\n",
    "    )\n",
    "    fig_metrics.show()\n",
    "    \n",
    "    # 2. Confusion Matrix Heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        confusion_matrix, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=metadata['classes'],\n",
    "        yticklabels=metadata['classes']\n",
    "    )\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Class Distribution\n",
    "    unique_true, counts_true = np.unique(y_test_decoded, return_counts=True)\n",
    "    unique_pred, counts_pred = np.unique(y_pred_decoded, return_counts=True)\n",
    "    \n",
    "    fig_dist = go.Figure()\n",
    "    fig_dist.add_trace(go.Bar(\n",
    "        name='True Labels',\n",
    "        x=unique_true,\n",
    "        y=counts_true,\n",
    "        marker_color='lightgreen'\n",
    "    ))\n",
    "    fig_dist.add_trace(go.Bar(\n",
    "        name='Predicted Labels',\n",
    "        x=unique_pred,\n",
    "        y=counts_pred,\n",
    "        marker_color='lightcoral'\n",
    "    ))\n",
    "    fig_dist.update_layout(\n",
    "        title='Class Distribution: True vs Predicted',\n",
    "        xaxis_title='Class',\n",
    "        yaxis_title='Count',\n",
    "        barmode='group'\n",
    "    )\n",
    "    fig_dist.show()\n",
    "    \n",
    "    # 4. Text Length Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(processed_df[f'{text_col}_length'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title('Text Length Distribution')\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations\n",
    "create_visualizations(metrics, confusion_matrix, y_test_decoded, y_pred_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Your Model\n",
    "\n",
    "Try your trained model on new text examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text: str, model_id: str, model_type: str) -> str:\n",
    "    \"\"\"Make prediction on new text\"\"\"\n",
    "    # Preprocess text\n",
    "    processed_text = preprocessor.preprocess_text(text)\n",
    "    \n",
    "    if model_type == 'sklearn':\n",
    "        model = trainer.models[model_id]\n",
    "        vectorizer = trainer.vectorizers[model_id]\n",
    "        text_vec = vectorizer.transform([processed_text])\n",
    "        prediction = model.predict(text_vec)[0]\n",
    "    else:\n",
    "        # Transformer model\n",
    "        model_info = trainer.models[model_id]\n",
    "        model = model_info['model']\n",
    "        tokenizer = model_info['tokenizer']\n",
    "        \n",
    "        inputs = tokenizer(processed_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    # Decode prediction\n",
    "    predicted_class = preprocessor.label_encoder.inverse_transform([prediction])[0]\n",
    "    return predicted_class\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"This product is amazing! I love it!\",\n",
    "    \"Terrible quality, waste of money.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"Best purchase I've ever made!\",\n",
    "    \"Poor customer service and defective item.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing your trained model:\\n\")\n",
    "\n",
    "for text in test_texts:\n",
    "    prediction = predict_text(text, model_id, model_type)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {prediction.upper()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully used AutoNLP-Agent to:\n",
    "\n",
    "‚úÖ **Upload and analyze** your dataset  \n",
    "‚úÖ **Automatically detect** the NLP task type  \n",
    "‚úÖ **Preprocess and clean** your text data  \n",
    "‚úÖ **Train a machine learning model** (with GPU acceleration if available)  \n",
    "‚úÖ **Evaluate performance** with comprehensive metrics  \n",
    "‚úÖ **Visualize results** with interactive charts  \n",
    "‚úÖ **Test predictions** on new text examples  \n",
    "\n",
    "### Key Features Demonstrated:\n",
    "- **No-code NLP**: Just upload data, get results\n",
    "- **Automatic task detection**: Intelligently identifies your use case\n",
    "- **GPU acceleration**: Leverages Colab's GPU for faster training\n",
    "- **Comprehensive evaluation**: Multiple metrics and visualizations\n",
    "- **Production-ready models**: Can be exported and deployed\n",
    "\n",
    "### Model Performance:\n",
    "- **Accuracy**: {metrics['accuracy']:.4f}\n",
    "- **F1-Score**: {metrics['f1_score']:.4f}\n",
    "- **Precision**: {metrics['precision']:.4f}\n",
    "- **Recall**: {metrics['recall']:.4f}\n",
    "\n",
    "### Next Steps:\n",
    "1. **Improve performance**: Try different models or hyperparameters\n",
    "2. **Deploy model**: Export for production use\n",
    "3. **Scale up**: Process larger datasets\n",
    "4. **Customize**: Add domain-specific preprocessing\n",
    "\n",
    "**AutoNLP-Agent** - Democratizing NLP through automation and simplicity! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}